{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1325a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define transformations for image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, image_dir, caption_file, transform=None, tokenizer=None, sample_size=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_ids = []\n",
    "        self.captions = []\n",
    "\n",
    "        # Read CSV with comma separator\n",
    "        with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)  # skip header\n",
    "            for row in reader:\n",
    "                if len(row) != 2:\n",
    "                    continue  # skip malformed lines\n",
    "                image_id, caption = row\n",
    "                self.image_ids.append(image_id)\n",
    "                self.captions.append(caption)\n",
    "        \n",
    "        # If sample_size is provided, randomly sample from the data\n",
    "        if sample_size:\n",
    "            sampled_indices = random.sample(range(len(self.image_ids)), min(sample_size, len(self.image_ids)))\n",
    "            self.image_ids = [self.image_ids[i] for i in sampled_indices]\n",
    "            self.captions = [self.captions[i] for i in sampled_indices]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        caption = self.captions[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_id)\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            raise FileNotFoundError(f\"Image {image_path} not found.\")\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Tokenize the caption using the BertTokenizer\n",
    "        if self.tokenizer:\n",
    "            tokenized_caption = self.tokenizer(\n",
    "                caption,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=50,\n",
    "                return_tensors='pt'\n",
    "            ).input_ids.squeeze(0)  # Convert to 1D tensor\n",
    "        else:\n",
    "            tokenized_caption = caption  # If no tokenizer is passed, return the caption as is\n",
    "        \n",
    "        return image, tokenized_caption\n",
    "\n",
    "# Paths\n",
    "image_dir = 'C:/Users/ajais/Downloads/LIS 640 Project/Dataset/Images'\n",
    "caption_file = 'C:/Users/ajais/Downloads/LIS 640 Project/Dataset/captions.txt'\n",
    "\n",
    "# Compute total number of lines (excluding header) to get total dataset size\n",
    "with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "    total_lines = sum(1 for _ in f) - 1  # subtract 1 for header\n",
    "\n",
    "# Use 10% of the dataset\n",
    "sample_size = int(total_lines * 0.1)\n",
    "\n",
    "# Initialize Dataset with tokenizer\n",
    "dataset = ImageCaptioningDataset(\n",
    "    image_dir=image_dir,\n",
    "    caption_file=caption_file,\n",
    "    transform=transform,\n",
    "    tokenizer=tokenizer,  # Pass tokenizer here\n",
    "    sample_size=sample_size\n",
    ")\n",
    "\n",
    "# Split dataset into training (70%), validation (15%), and test (15%)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Initialize DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682d87eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False  # Freeze ResNet weights\n",
    "\n",
    "        modules = list(resnet.children())[:-1]  # Remove the last FC layer\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)  # Output shape: (batch_size, 2048, 1, 1)\n",
    "        features = features.view(features.size(0), -1)  # Shape: (batch_size, 2048)\n",
    "        features = self.fc(features)  # Shape: (batch_size, embed_size)\n",
    "        features = self.bn(features)\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b0dfda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers, max_len=50):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, max_len, embed_size))  # Learnable positional encoding\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=hidden_dim)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)\n",
    "    \n",
    "    def forward(self, tgt, memory):\n",
    "        # tgt: (batch_size, seq_len)\n",
    "        # memory: (batch_size, seq_len, embed_size)\n",
    "        \n",
    "        # Add positional encoding to the target embeddings\n",
    "        tgt_emb = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
    "        tgt_emb = tgt_emb.permute(1, 0, 2)  # (seq_len, batch_size, embed_size)\n",
    "        \n",
    "        # Ensure memory is in the correct shape\n",
    "        memory = memory.permute(1, 0, 2)  # (seq_len, batch_size, embed_size)\n",
    "        \n",
    "        # Pass through the transformer decoder\n",
    "        output = self.transformer_decoder(tgt_emb, memory)\n",
    "        \n",
    "        # Output layer to predict the next word in the sequence\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        # Return the output in shape (batch_size, seq_len, vocab_size)\n",
    "        return output.permute(1, 0, 2)  # (batch_size, seq_len, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cbc1f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajais\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ajais\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embed_size = 256\n",
    "num_heads = 4\n",
    "hidden_dim = 256\n",
    "num_layers = 2\n",
    "\n",
    "# Instantiate encoder and decoder\n",
    "encoder = CNNEncoder(embed_size)\n",
    "decoder = TransformerDecoder(vocab_size, embed_size, num_heads, hidden_dim, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610de14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | encoder   | CNNEncoder         | 24.0 M | train\n",
      "1 | decoder   | TransformerDecoder | 17.0 M | train\n",
      "2 | criterion | CrossEntropyLoss   | 0      | train\n",
      "---------------------------------------------------------\n",
      "17.5 M    Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "41.0 M    Total params\n",
      "164.090   Total estimated model params size (MB)\n",
      "187       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8bd7dc2e20491aabad9667fd310e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajais\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\ajais\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae93dc377744349af467f4192f2d2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7374462fbb3343158edacf845c40e5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fd327ab87243e782f6aa9cec9505b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6bcf868e7f44b9811eae92c172e606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1186cc70410e4cd1a1d756607ad70db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fedb2b5df04f4d169d32177336412080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc954be628b41bbafb22be0a700583b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af0e05b4655439e8eb570a65fbf41e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879ffae7e77f42b1b2154e246103eff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755480be11c141208729f909c4dcca0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944399215cd94e0d98b5b8262d2028f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "c:\\Users\\ajais\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f551964be7f47d78175a4d5bf47a6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6731557846069336     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2.533141613006592     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6731557846069336    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.533141613006592    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 2.533141613006592, 'test_acc': 0.6731557846069336}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "class LitImageCaptioningModel(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder, tokenizer, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, images, captions):    \n",
    "        with torch.no_grad():\n",
    "            features = self.encoder(images)  # (batch_size, embed_size)\n",
    "            seq_len = captions.shape[1]\n",
    "            features = features.unsqueeze(1).repeat(1, seq_len, 1)  # (batch_size, seq_len, embed_size)\n",
    "        \n",
    "        outputs = self.decoder(captions, features)\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, captions = batch\n",
    "        outputs = self(images, captions[:, :-1])\n",
    "        targets = captions[:, 1:]\n",
    "\n",
    "        loss = self.criterion(\n",
    "            outputs.reshape(-1, outputs.shape[-1]),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        preds = outputs.argmax(dim=-1)\n",
    "        mask = targets != self.tokenizer.pad_token_id\n",
    "        correct = (preds == targets) & mask\n",
    "        acc = correct.sum().float() / mask.sum().float()\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, captions = batch\n",
    "        inputs = captions[:, :-1]\n",
    "        targets = captions[:, 1:]\n",
    "\n",
    "        outputs = self(images, inputs)\n",
    "        outputs = outputs[:, : targets.shape[1], :]\n",
    "\n",
    "        loss = self.criterion(\n",
    "            outputs.reshape(-1, outputs.shape[-1]),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        preds = outputs.argmax(dim=-1)\n",
    "        mask = targets != self.tokenizer.pad_token_id\n",
    "        correct = (preds == targets) & mask\n",
    "        acc = correct.sum().float() / mask.sum().float()\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, captions = batch\n",
    "        inputs = captions[:, :-1]\n",
    "        targets = captions[:, 1:]\n",
    "\n",
    "        outputs = self(images, inputs)\n",
    "        outputs = outputs[:, : targets.shape[1], :]\n",
    "\n",
    "        loss = self.criterion(\n",
    "            outputs.reshape(-1, outputs.shape[-1]),\n",
    "            targets.reshape(-1)\n",
    "        )\n",
    "\n",
    "        preds = outputs.argmax(dim=-1)\n",
    "        mask = targets != self.tokenizer.pad_token_id\n",
    "        correct = (preds == targets) & mask\n",
    "        acc = correct.sum().float() / mask.sum().float()\n",
    "\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"test_acc\", acc, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "# Logger\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"image_captioning\")\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"cpu\",\n",
    "    logger=logger,\n",
    "    log_every_n_steps=5,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "# Instantiate the model\n",
    "lit_model = LitImageCaptioningModel(encoder, decoder, tokenizer)\n",
    "\n",
    "# Train\n",
    "trainer.fit(lit_model, train_dataloader, val_dataloader)\n",
    "\n",
    "# Test\n",
    "trainer.test(lit_model, dataloaders=test_dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
